"""
Functions for loading largescale datasets, performing data splitting and constructing confidence intervals.
"""

import sys, os
import numpy as np
import pandas as pd

import itertools
import pdb
from tqdm import tqdm

from scipy.special import binom
from scipy.optimize import fsolve

from collections import defaultdict
from numpy.random import default_rng
from dataclasses import dataclass
from typing import Union, List, Set, Dict, Any, Tuple, Callable, Optional
from numpy.typing import NDArray

from .largescale_utils import default_weight


@dataclass(frozen=True)
class DataConfig:
    """
    A read-only container for data attributes generated by DataSplitter.
    It holds all observed ratings as dictionary structured by sampling dimension (by-user or by-item),
    data dimensions, user/item raw ids, sample counts etc.

    Being a frozen dataclass, its instances cannot be modified after creation.
    """
    key_to_samples: Dict[Any, List[Tuple]]
    key_sample_counts: Dict[Any, int]
    user_ids: Set[Any]
    item_ids: Set[Any]
    n_users: int
    n_items: int
    n_ratings: int
    sampling_dim: str


class DataSplitter:
    """Handles data loading, splitting, and query sampling for matrix completion.

    This class reads interaction data once and builds a single efficient dictionary
    lookup (either user-centric or item-centric) to enable scalable sampling.

    Parameters
    ----------
    file_path : str
        Path to the ratings file.
    reader : Reader
        A reader object with a `parse_line` method.
    sampling_dim : {'user', 'item'}, optional
        The dimension to use as keys for grouping interactions. This choice is 
        fixed upon initialization. Defaults to 'item'.
    """
    
    def __init__(self, file_path, reader, sampling_dim='item',
                verbose=True, progress=True):
        if not os.path.isfile(file_path):
            raise FileNotFoundError(f"File {file_path} not found")
        if sampling_dim not in ('user', 'item'):
            raise ValueError("sampling_dim must be either 'user' or 'item'")
        
        self.file_path = file_path
        self.reader = reader
        self.sampling_dim = sampling_dim
        self.verbose = verbose
        self.progress = progress

        self.config: DataConfig = None

        # Directly build the single required dictionary upon initialization.
        self._load_and_build_dict()
    

    def _load_and_build_dict(self):
        """Reads the ratings file once and builds one dictionary mapping keys to entities."""
        key_to_samples = defaultdict(list)
        key_sample_counts = defaultdict(int) 
        n_ratings = 0
        
        with open(os.path.expanduser(self.file_path)) as f:
            ratings_iterator = (
                self.reader.parse_line(line)
                for line in itertools.islice(f, self.reader.skip_lines, None)
            )

            user_id_set = set()
            item_id_set = set()

            for urid, irid, r in tqdm(ratings_iterator, leave=True, position=0, 
                      disable = not self.progress):
                n_ratings += 1
                user_id_set.add(urid)
                item_id_set.add(irid)

                if self.sampling_dim == 'user':
                    key_to_samples[urid].append((irid, r))
                    key_sample_counts[urid] += 1
                else:
                    key_to_samples[irid].append((urid, r))
                    key_sample_counts[irid] += 1


        self.config = DataConfig(
            key_to_samples=key_to_samples,
            key_sample_counts=key_sample_counts,
            user_ids= user_id_set,
            item_ids= item_id_set,
            n_users=len(user_id_set),
            n_items=len(item_id_set),
            n_ratings=n_ratings,
            sampling_dim=self.sampling_dim
        )

        if self.verbose:
            print("Loaded {} ratings from {} users and {} items.".format(self.config.n_ratings, self.config.n_users, self.config.n_items))
            sys.stdout.flush()


    def _validate_query_size(self, query_size, possible_query_num, max_n_query, param_name):
        """
        validation helper to check if the query size is meaningful
        """
        query_size_type = np.asarray(query_size).dtype.kind
        
        if max_n_query is not None:
            max_n_query = int(np.clip(max_n_query, 1, possible_query_num))
        else:
            max_n_query = possible_query_num

        if possible_query_num == 0:
            raise Exception('Number of possible query is 0, try a smaller group size k.')

        if (
            query_size_type == "i"
            and (query_size >= possible_query_num or query_size <= 0)
            or query_size_type == "f"
            and (query_size <= 0 or query_size >= 1)
        ):
            raise ValueError(
                "{0}_size={1} should be either positive and smaller"
                " than the maximum possible number {2} or a float in the "
                "(0, 1) range".format(param_name, query_size, possible_query_num)
            )
        
        if query_size is not None and query_size_type not in ("i", "f"):
            raise ValueError("Invalid value for {0}_size: {1}".format(param_name, query_size))
        
        if query_size_type == "f":
            n_query = int(query_size * possible_query_num)
        elif query_size_type == "i":
            n_query = query_size
        elif query_size is None:
            n_query = int(0.5 * possible_query_num)
        
        return np.min([n_query, max_n_query])


    def _key2list(self, key, samples):
        if self.sampling_dim == 'user':
            sample_list = [(key, item, rating) for (item, rating) in samples]
        else:
            sample_list = [(user, key, rating) for (user, rating) in samples]
        return sample_list


    def sample_train_calib(self, k, calib_size=0.5, max_n_calib=2000, random_state=0):
        """Splits observed ratings into training and calibration sets.

        The sampling dimension is determined by the `sampling_dim` parameter
        provided during the initialization of the class. 

        Parameters
        ----------
        k : int
            The size of each query.
        calib_size : float or int
            The proportion or number of queries for the calibration set.
        random_state : int
            Seed for the random number generator.

        Returns
        -------
        train_samples : list of tuples
            (user_id, item_id, rating) tuples for the training set.
        calib_samples : list of tuples
            (user_id, item_id, rating) tuples for the calibration set.
        """
        rng = default_rng(random_state)

        # Create a temporary copy for shuffling and sampling.
        avail_samples = {key: list(samples) for key, samples in self.config.key_to_samples.items()}
        avail_sample_counts = {}
        possible_query_num = 0 
        train_samples = []
        calib_samples = []

        # Trim the number of samples per key to be multiple of k
        for key in self.config.key_sample_counts.keys():
            if self.config.key_sample_counts[key] >= k:
                rng.shuffle(avail_samples[key])

                possible_query_num += int(self.config.key_sample_counts[key] // k)
                n_trim = int(self.config.key_sample_counts[key] % k)

                train_samples.extend(self._key2list(key, avail_samples[key][:n_trim]))
                avail_samples[key] = avail_samples[key][n_trim:]
                avail_sample_counts[key] = len(avail_samples[key])
            else:
                train_samples.extend(self._key2list(key, avail_samples[key]))
                del avail_samples[key]
        
        # Verify cablib query size
        n_queries = self._validate_query_size(calib_size, possible_query_num, max_n_calib, "calib")


        for _ in tqdm(range(n_queries)):
            keys, prob = list(avail_sample_counts.keys()), list(avail_sample_counts.values())
            
            prob /= np.sum(prob)
            chosen_key_idx = rng.choice(len(keys), p=prob)
            chosen_key = keys[chosen_key_idx]
            
            # First k shuffled entries form a calibration query
            calib_samples.extend(self._key2list(chosen_key, avail_samples[chosen_key][:k]))

            # Update the available indices by removing the selected query
            if avail_sample_counts[chosen_key] == k:
                del avail_sample_counts[chosen_key]
                del avail_samples[chosen_key]
            else:
                avail_sample_counts[chosen_key] -= k
                avail_samples[chosen_key] = avail_samples[chosen_key][k:]
            

        # Add all remaining samples to traning set.
        for key, samples in avail_samples.items():
            train_samples.extend(self._key2list(key, samples))

        # Sanity check: make sure calib and train ratings equal to total ratings
        assert len(train_samples) + len(calib_samples) == self.config.n_ratings, "Check query sampling process, calib + train ratings not equal to total ratings!"

        # Check for missing users/items in the training set ---
        if train_samples: # Only check if the training set is not empty
            original_user_ids = set(self.config.user_ids)
            original_item_ids = set(self.config.item_ids)

            # Use efficient set comprehensions to get IDs from the training data
            train_user_ids = {uid for uid, _, _ in train_samples}
            train_item_ids = {iid for _, iid, _ in train_samples}

            missing_users = original_user_ids - train_user_ids
            if missing_users:
                warnings.warn(
                    f"The training set is missing {len(missing_users)} users: {missing_users}. "
                    "This can happen if all ratings for a user were allocated to the calibration set. "
                    f"Consider using a smaller 'calib_size'. Missing user count: {len(missing_users)}",
                    UserWarning
                )

            missing_items = original_item_ids - train_item_ids
            if missing_items:
                warnings.warn(
                    f"The training set is missing {len(missing_items)} items: {missing_items}. "
                    "This can happen if all ratings for an item were allocated to the calibration set. "
                    f"Consider using a smaller 'calib_size'. Missing item count: {len(missing_items)}",
                    UserWarning
                )

        return train_samples, calib_samples, [missing_users, missing_items]


class SimulCI_ls():
    """ 
    This class computes the simultaneous conformal prediction region for test query with length k
    Designed for handling largescale data. 
    """
    def __init__(self, data_config: DataConfig, calib_samples: list, k: int,
                 Mhat: Callable[[List[Tuple]], Tuple[List[float], List[bool]]],
                 w_obs: Callable[[List[Tuple]], Tuple[List[float], List[bool]]],
                 verbose=True, progress=True):
        self.config = data_config
        self.k = k
        self.verbose = verbose
        self.progress = progress

        # Callables for point predictions
        self.Mhat = Mhat
        self.w_obs = w_obs

        # Pre-compute some fixed quantities
        self.key_dim = 0 if self.config.sampling_dim == "user" else 1
        self.n_obs = self.config.key_sample_counts   # Number of observations for each key
        # Number of observations for each key after random dropping
        self.n_obs_drop = {key: count - count % self.k for key, count in self.n_obs.items()}     
        # Number of missing entries for each key
        n_entries = self.config.n_users if self.key_dim else self.config.n_items                       
        self.n_miss = {key: n_entries - count for key, count in self.n_obs.items()}    
        
        # Process calibration queries
        if not calib_samples:
            raise ValueError("Calibration set is empty!")
        else:
            # Separate calibration samples into indices and ratings
            uids, iids, rs = zip(*calib_samples)
            self.idxs_calib = list(zip(uids, iids))
            self.n_calib_queries = len(self.idxs_calib)//self.k
            self.keys_calib = [self.idxs_calib[self.k * i][self.key_dim] for i in range(self.n_calib_queries)]

            # Compute the calibration scores
            self._get_calib_scores(rs)
            self.calib_order = np.argsort(self.calib_scores)
            self.st_calib_scores = self.calib_scores[self.calib_order]


    def _get_calib_scores(self, ratings):
        """
        This function implements the scores as the maximum of the absolute prediction errors
        """
        preds, was_impossible = self.Mhat(self.idxs_calib)
        if np.any(was_impossible):
            warnings.warn(
                f"Predictions for {np.sum(was_impossible)} calibrations pairs were impossible. Using default.",
                UserWarning
            )

        abs_err = np.abs(np.array(preds)-np.array(ratings))
        self.calib_scores = np.array([np.max(abs_err[self.k*i: self.k*(i+1)]) for i in range(self.n_calib_queries)])
    

    def _initialize_df(self, a_list, n):
        """
        Helper that initializes the confidence interval DataFrame
        """
        df = pd.DataFrame({})

        df["alpha"] = a_list
        df["lower"] = [np.zeros(n) for _ in a_list]
        df["upper"] = [np.zeros(n) for _ in a_list]
        df["is_inf"] = [np.zeros(n) for _ in a_list]
        df["is_impossible"] = [np.zeros(n) for _ in a_list]

        return df
    

    def _compute_universal(self, w_test):
        # Universal quantities invariant to the test query
        sr_miss = {}     # Sum of test weights for missing set by key
        sr_prune = {}    # Sum of test weights for pruned missing set by key
        sw_prune = 0.0   # Sum of test weights for pruned missing set
        delta = 0.0      # Sum of sampling weights on the missing set
        all_observed_weights = []

        if self.verbose:
            print("Start computing universal quantities invariant to the test query...")
            sys.stdout.flush()
        
        for key in tqdm(self.n_obs.keys(), desc="Universal quantities", leave=True, position=0, 
                      disable = not self.progress):
            obs_entries = set([entry for entry, rating in self.config.key_to_samples[key]])
            all_entries = self.config.user_ids if self.key_dim else self.config.item_ids
            miss_entries = all_entries - obs_entries
            
            # Update the quantities per key  
            if self.key_dim == 0:
                sr_miss[key] = np.sum(w_test([(key, entry) for entry in miss_entries]))
            else:
                sr_miss[key] = np.sum(w_test([(entry, key) for entry in miss_entries]))
            
            sr_prune[key] = 0 if self.n_miss[key] < self.k else sr_miss[key]
            sw_prune += sr_prune[key]
            if self.key_dim == 0:
                delta += np.sum(np.array(self.w_obs([(key, entry) for entry in miss_entries])[0]))
            else:
                delta += np.sum(np.array(self.w_obs([(entry, key) for entry in miss_entries])[0]))

            if obs_entries:
                if self.key_dim == 0:
                    observed_weights_for_key = self.w_obs([(key, entry) for entry in obs_entries])[0]
                else:
                    observed_weights_for_key = self.w_obs([(entry, key) for entry in obs_entries])[0]
                all_observed_weights.extend(observed_weights_for_key)

        def _z(r, w_array, d):
            denominators = np.power(2, r * w_array) - 1
            safe_denominators = np.where(denominators == 0, 1e-9, denominators)   
            sum_term = np.sum(w_array / safe_denominators)
            return d - (1/r) - sum_term

        initial_guess = 1 / delta if delta != 0 else 1.0
        scale = fsolve(_z, initial_guess, args=(np.array(all_observed_weights), delta))[0]

        if self.verbose:
            print("Done!")
            sys.stdout.flush()

        return sr_prune, sw_prune, sr_miss, delta, scale


    def _weight_single(self, idx_test, key_test, w_test, 
                       sr_prune, sw_prune, sr_miss, delta, scale,
                       store_weights = False):
        weights = np.ones(self.n_calib_queries + 1)

        # Sum of test sampling weights for the test query
        sw_test = np.sum(w_test[idx_test])
        # Sum of observation sampling weights for the test query
        w_obs_test = self.w_obs[idx_test][0]
        sw_obs_test = np.sum(np.array(w_obs_test))

        # Augmented index set
        idxs_full = self.idxs_calib + idx_test
        keys_full = self.keys_calib + key_test

        # Compute the quantile inflation weights
        for i in range(self.n_calib_queries + 1):
            idx_i = idxs_full[self.k*i : self.k*(i+1)]
            key_i = keys_full[i]
            csum_i = np.cumsum(w_test[idx_i])

            # Compute the prob of the ith query being the test query
            prob_test = np.prod(w_test[idx_i])    # the numerator

            sw_diff = 0
            if key_i != key_test:
                if self.n_miss[key_i] < self.k:
                    sw_diff += sr_miss[key_i]
                if self.n_miss[key_test] < 2*self.k:
                    sw_diff -= (sr_miss[key_test] - sw_test)
            prob_test /= (sw_prune - sw_test + csum_i[-1] + sw_diff)

            for j in range(1,self.k):
                sw_diff = 0
                if key_i == key_test:
                    sw_diff -= sw_test
                prob_test /= (sr_miss[key_i] + csum_i[-1] - csum_i[j-1] + sw_diff)

            # Compute the prob of sampling the given calibration queries
            prob_cal = 1
            if key_i != key_test:
                prob_cal *= binom(self.n_obs[key_i], self.n_obs_drop[key_i])
                prob_cal /= binom(self.n_obs[key_i]-self.k, self.n_obs_drop[key_i]-self.k)
                prob_cal *= binom(self.n_obs[key_test], self.n_obs_drop[key_test])
                prob_cal /= binom(self.n_obs[key_test]+self.k, self.n_obs_drop[key_test]+self.k)

                for j in range(1, self.k):
                    prob_cal *= (self.n_obs_drop[key_i]-j)/(self.n_obs_drop[key_test]+self.k-j)

            # Compute the prob of sampling the given observation entries
            w_obs_i = self.w_obs[idx_i][0]
            diff = np.sum(np.array(w_obs_i)) - sw_obs_test
            prob_obs = 0.5**(scale * diff)*(delta + diff)/delta
            for j in range(self.k):
                prob_obs *= 1 - 0.5**(scale * w_obs_test[j])
                prob_obs /= 1 - 0.5**(scale * w_obs_i[j])

            # Assemble the final weight
            weight[i] = prob_test * prob_cal * prob_obs
        
        weights /= np.sum(weights)

        if store_weights:
            self.weights_list.append(weights)
        
        return weights[:-1]


    def get_CI(self, idxs_test: list, alpha: Union[float, List[float]], 
               w_test: Optional[Callable[[List[Tuple]], NDArray[np.float64]]] = None, 
               allow_inf=True, store_weights=False):
        """
        Compute the confidence intervals for all test queries at any confidence levels

        Parameters
        ----------
        idxs_test: List of (user_id, item_id) tuples where IDs are raw IDs
            e.g., [(uid1, iid1), (uid2, iid2), ...]
        alpha : float or list of floats indicating significance level, e.g. [0.05, 0.1]
            For each alpha, computes confidence intervals for all test points.
        w_test : Callable, optional
            Callable that maps list of index tuples e.g., [(uid1, iid1), (uid2, iid2), ...]
            to test sampling weights.
        allow_inf: bool
            If True, the output intervals might be (-np.inf, np.inf).
        store_weights: bool
            If True, the quantile inflation weights for the test points are stored in the class
            object. Default is False, only toggle on if one needs to access / analyze the weights
            directly. 

        Returns
        -------
        DataFrame
            a pandas DataFrame with 5 columns: alpha, lower, upper, is_inf, is_impossible 
            alpha: numeric value of confidence level.
            lower: array of lower confidence bounds.
            upper: array of upper confidence bounds.
            is_inf: array of bool where 1 indicates the method outputs (-np.inf, np.inf).
            is_impossible: array of bool where 1 indicate impossible pair where user/item index is unseen
                by either Mhat or w_obs callable.
        """
        n_test_queries = len(idxs_test)//self.k
        a_list = np.array(alpha).reshape(-1)
        
        df = self._initialize_df(a_list=a_list, n = n_test_queries * self.k)
        
        if store_weights:
            self.weights_list = []
        
        if w_test == None:
            w_test = default_weight  # Uniform test sampling weights

        if self.verbose:
            print("Computing conformal prediction intervals for {} test queries...".format(n_test_queries))
            sys.stdout.flush()

        # Compute constant components in weights that are invariant to test query
        # [Note] Only need once for each set of w_test, move outside if working with batches of testing points 
        #   with the same w_test.
        sr_prune, sw_prune, sr_miss, delta, scale = self._compute_universal(w_test)  

        for i in tqdm(range(n_test_queries), desc="CI", leave=True, position=0, 
                      disable = not self.progress):
            idx_test = idxs_test[self.k*i : self.k*(i+1)]
            key_test = idx_test[0][self.key_dim]

            weights = self._weight_single(idx_test, key_test, w_test, sr_prune, sw_prune, sr_miss, delta, scale, store_weights)

            cweights = np.cumsum(weights[self.calib_order])
            est, is_impossible_est = self.Mhat(idx_test)
            _, is_impossible_prob = self.w_obs(idx_test)
            # Test pair is impossible to predict if it contains user/item unseen in training set for w_obs or Mhat.
            est, is_impossible = np.array(est), np.array(is_impossible_est)|np.array(is_impossible_prob)

            for row, a in enumerate(a_list):
                df.loc[row, "is_impossible"][self.k*i : self.k*(i+1)] = is_impossible
                if cweights[-1] < 1-a:
                    df.loc[row, "is_inf"][self.k*i : self.k*(i+1)] = 1
                    if allow_inf:
                        df.loc[row, "lower"][self.k*i : self.k*(i+1)] = -np.inf
                        df.loc[row, "upper"][self.k*i : self.k*(i+1)] = np.inf
                    else:
                        df.loc[row, "lower"][self.k*i : self.k*(i+1)] = est - self.st_calib_scores[-1]
                        df.loc[row, "upper"][self.k*i : self.k*(i+1)] = est + self.st_calib_scores[-1]
                else:
                    idx = np.argmax(cweights >= 1-a)
                    df.loc[row, "lower"][self.k*i : self.k*(i+1)] = est - self.st_calib_scores[idx]
                    df.loc[row, "upper"][self.k*i : self.k*(i+1)] = est + self.st_calib_scores[idx]
            
        if self.verbose:
            print("Done!")
            sys.stdout.flush()
            
        return df